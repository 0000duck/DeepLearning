{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premade Estimators\n",
    "\n",
    "**requirements**\n",
    "\n",
    "    tensorflow==1.7.0\n",
    "\n",
    "Geting the right version of tensorflow by\n",
    "\n",
    "    $ pip install --upgrade tensorflow==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the tensorflow high-level API:\n",
    "\n",
    "  * [Estimator](https://www.tensorflow.org/programmers_guide/estimators), which represent a complete model. The Estimator API provides methods to train the model, to judge the model's accuracy, and to generate predictions.\n",
    "    \n",
    "  * [Database](https://www.tensorflow.org/get_started/datasets_quickstart), which build a data input pipeline. The Dataset API has methods to load and manipulate data, and feed it into your model. The Dataset API meshes well with the Estimators API.\n",
    "\n",
    "To write a TensorFlow program based on pre-made Estimators, you must perform the following tasks:\n",
    "\n",
    "  * Create one or more input functions.\n",
    "  * Define the model's feature columns.\n",
    "  * Instantiate an Estimator, specifying the feature columns and various hyperparameters.\n",
    "  * Call one or more methods on the Estimator object, passing the appropriate input function as the source of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load eample dataset\n",
    "def input_evaluation_set():\n",
    "    features = pd.DataFrame({'SepalLength': np.array([6.4, 5.0, 3.2, 6.5]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3, 5.3, 6.4]),\n",
    "                'PetalLength': np.array([5.6, 3.3, 6.4, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0, 6.4, 1.3])})\n",
    "    labels = np.array([2, 1, 0, 2])\n",
    "    return features, labels\n",
    "\n",
    "def input_train_set():\n",
    "    features = pd.DataFrame({'SepalLength': np.array([3.2, 5.3]),\n",
    "                'SepalWidth':  np.array([3.8, 4.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])})\n",
    "    labels = np.array([1, 2])\n",
    "    return features, labels\n",
    "\n",
    "train_X, train_y = input_train_set()\n",
    "test_X, test_y = input_evaluation_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input functions\n",
    "\n",
    "An input function is a function that returns a `tf.data.Dataset` object which outputs the following two-element tuple:\n",
    "\n",
    "  * feature - A python dict in which\n",
    "    * Each key is the name of a feature.\n",
    "    * Each value in an array containing all of that feature's values.\n",
    "  \n",
    "  * label - An array containing the values of the label for every example.\n",
    "\n",
    "`tf.data.Dataset` is an Iterator Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of input function\n",
    "def my_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature columns\n",
    "\n",
    "A [feature column](https://developers.google.com/machine-learning/glossary/#feature_columns) is an object describing how the model should use raw input data from the features dictionary.  The [tf.feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column) module provides many options for representing data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train_X.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate an estimator\n",
    "\n",
    "TensorFlow provides several pre-made classifier Estimators, including:\n",
    "\n",
    "  * `tf.estimator.DNNClassifier` for deep models that perform multi-class classification.\n",
    "  * `tf.estimator.DNNLinearCombinedClassifier` for wide & deep models.\n",
    "  * `tf.estimator.LinearClassifier` for classifiers based on linear models\n",
    "  \n",
    "`tf.estimator.DNNClassifier` seems like the best choice. Here's how we instantiated this Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/b6/4pljczw11w10f2tqrk_5gnp00000gn/T/tmpmb9l2at8\n"
     ]
    }
   ],
   "source": [
    "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Evaluate, and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x117e98080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "classifier.train(\n",
    "    input_fn=lambda:my_input_fn(train_X, train_y, 1),\n",
    "    steps=10  #  tells the method to stop training after a number of training steps.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_y = classifier.evaluate(\n",
    "    input_fn = lambda: my_input_fn(test_X, test_y, batch_size=1)\n",
    ")\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_y = classifier.predict(\n",
    "    input_fn = lambda: my_input_fn(test_X, test_y, batch_size=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': array([-0.97360694, -1.5043912 ,  1.5658324 ], dtype=float32), 'probabilities': array([0.07012276, 0.04124224, 0.88863504], dtype=float32), 'class_ids': array([2]), 'classes': array([b'2'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "print(next(pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
