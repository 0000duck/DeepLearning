{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7: Develop Your First Neural Network With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Overview\n",
    "\n",
    "Six steps:\n",
    "\n",
    "1. Load Data.\n",
    "2. Define Model.\n",
    "3. Compile Model.\n",
    "4. Fit Model.\n",
    "5. Evaluate Model.\n",
    "6. Tie It All Together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Get Data Set\n",
    "\n",
    "In this tutorial we are going to use the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "The introduction of this data set: [pima-indians-diabetes.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names)\n",
    "\n",
    "The data set file: [pima-indians-diabetes.data](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data)\n",
    "\n",
    "You can get the data by the fellowing step:\n",
    "\n",
    "```\n",
    "$ cd ./data_set/\n",
    "$ ./get_pima_indians_diabetes_data.sh\n",
    "```\n",
    "\n",
    "pima-indians-diabetes.data will be downloaded at data_set/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Load Data\n",
    "\n",
    "### Initialize random seed\n",
    "\n",
    "It is a good idea to initialize the random number generator with a fixed seed value. This is so that you can run the same code again and again and get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "\n",
    "Load data by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (768, 8) float64\n",
      "Y (768,) float64\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt(\"./data_set/pima-indians-diabetes.data\", delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "print 'X', X.shape, X.dtype\n",
    "print 'Y', Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Define Model\n",
    "\n",
    "Create a model by keras.models.Sequential and add the layers we designed:\n",
    "\n",
    "    The first hidden layer has 12 neurons and expects 8 input variables, and a relu activation.\n",
    "    The second hidden layer has 8 neurons, and a relu activation.\n",
    "    Finally the output layer has 1 neuron to predict the class, and a sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model\n",
    "\n",
    "Compiling the model uses the efficient numercial libraries (Tensorflow or Theano). In this step, we shoule specify some hyperperemeters for training process:\n",
    "\n",
    "    loss function: binary_crossentropy\n",
    "    optimizer: adam\n",
    "    metrics\" accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Fit Model\n",
    "\n",
    "Fit moel means training model, the peremeters in this step are:\n",
    "\n",
    "    nb_epoch: 150\n",
    "    batch_size: 10\n",
    "\n",
    "nb_epoch means the number of epoch, which fix the number of iterations. batch_size means the batch size in the method \"mini-batch gradient descent\"\n",
    "\n",
    "The training process is runing on your CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 0.6840 - acc: 0.6120     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.6654 - acc: 0.6510     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.6536 - acc: 0.6510     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6419 - acc: 0.6484     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6258 - acc: 0.6628     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6150 - acc: 0.6797     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6086 - acc: 0.6732     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6014 - acc: 0.6836     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.5970 - acc: 0.6901     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.5911 - acc: 0.6849     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.5910 - acc: 0.6875     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.5886 - acc: 0.7031     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.5870 - acc: 0.6901     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.5803 - acc: 0.6966     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.5768 - acc: 0.6992     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.5727 - acc: 0.7031     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5863 - acc: 0.6992     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5827 - acc: 0.6940     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5798 - acc: 0.7044     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5776 - acc: 0.6888     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5763 - acc: 0.7057     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5744 - acc: 0.6940     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5721 - acc: 0.6953     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5724 - acc: 0.7005     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5718 - acc: 0.6849     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5658 - acc: 0.7018     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5648 - acc: 0.7174     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5718 - acc: 0.7031     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5667 - acc: 0.7044     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5639 - acc: 0.7109     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5667 - acc: 0.7083     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5623 - acc: 0.7096     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5601 - acc: 0.7201     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5554 - acc: 0.7240     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5576 - acc: 0.7174     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5564 - acc: 0.7148     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5571 - acc: 0.7214     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5531 - acc: 0.7187     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5558 - acc: 0.7174     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5544 - acc: 0.7253     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5512 - acc: 0.7214     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5510 - acc: 0.7161     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5564 - acc: 0.7188     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5474 - acc: 0.7357     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5521 - acc: 0.7109     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5495 - acc: 0.7240     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5435 - acc: 0.7253     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5448 - acc: 0.7240     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5404 - acc: 0.7435     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5404 - acc: 0.7240     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5396 - acc: 0.7318     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5388 - acc: 0.7357     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5395 - acc: 0.7253     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5388 - acc: 0.7305     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5335 - acc: 0.7357     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5372 - acc: 0.7344     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5362 - acc: 0.7357     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5304 - acc: 0.7383     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5312 - acc: 0.7435     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5282 - acc: 0.7370     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5303 - acc: 0.7474     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5260 - acc: 0.7604     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5290 - acc: 0.7240     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5258 - acc: 0.7383     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5224 - acc: 0.7513     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5178 - acc: 0.7513     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5246 - acc: 0.7448     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5193 - acc: 0.7591     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5268 - acc: 0.7552     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5233 - acc: 0.7435     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5220 - acc: 0.7617     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5165 - acc: 0.7552     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.5197 - acc: 0.7461     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5238 - acc: 0.7370     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5188 - acc: 0.7422     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5165 - acc: 0.7565     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5163 - acc: 0.7526     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.5144 - acc: 0.7461     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5107 - acc: 0.7591     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5111 - acc: 0.7552     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5088 - acc: 0.7474     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5113 - acc: 0.7513     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.5122 - acc: 0.7513     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.5100 - acc: 0.7422     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5042 - acc: 0.7604     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5071 - acc: 0.7552     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.5084 - acc: 0.7656     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.5082 - acc: 0.7656     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.5030 - acc: 0.7526     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5080 - acc: 0.7474     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5049 - acc: 0.7604     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.5029 - acc: 0.7526     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.5047 - acc: 0.7435     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.5059 - acc: 0.7539     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.4974 - acc: 0.7617     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.5034 - acc: 0.7617     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.5087 - acc: 0.7565     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4957 - acc: 0.7643     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4995 - acc: 0.7643     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4934 - acc: 0.7643     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4940 - acc: 0.7630     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4871 - acc: 0.7656     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4948 - acc: 0.7643     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4985 - acc: 0.7539     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.4910 - acc: 0.7682     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4923 - acc: 0.7604     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4931 - acc: 0.7721     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7552     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4880 - acc: 0.7721     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4905 - acc: 0.7760     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4857 - acc: 0.7760     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4940 - acc: 0.7526     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4832 - acc: 0.7591     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4913 - acc: 0.7682     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4913 - acc: 0.7591     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4826 - acc: 0.7812     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4825 - acc: 0.7721     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4896 - acc: 0.7578     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4977 - acc: 0.7565     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4818 - acc: 0.7617     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4885 - acc: 0.7643     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4818 - acc: 0.7721     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4876 - acc: 0.7669     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4787 - acc: 0.7669     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4830 - acc: 0.7721     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4783 - acc: 0.7643     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4803 - acc: 0.7721     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4824 - acc: 0.7617     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4770 - acc: 0.7695     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4794 - acc: 0.7708     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4812 - acc: 0.7656     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4755 - acc: 0.7643     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4812 - acc: 0.7682     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4707 - acc: 0.7747     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4786 - acc: 0.7826     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4767 - acc: 0.7865     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4746 - acc: 0.7695     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4744 - acc: 0.7799     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4719 - acc: 0.7669     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4792 - acc: 0.7708     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4715 - acc: 0.7773     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4887 - acc: 0.7773     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4715 - acc: 0.7721     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4850 - acc: 0.7721     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4662 - acc: 0.7773     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4768 - acc: 0.7695     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4686 - acc: 0.7786     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4653 - acc: 0.7826     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4693 - acc: 0.7669     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4696 - acc: 0.7734     \n",
      "Fit time Cost 6.95693182945 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# fit model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "end_time = time.time()\n",
    "print \"Fit time Cost %s s\"%(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.7 Evaluate Model\n",
    "\n",
    "In this part, we can calculate the accuracy fo this model on training dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0sTraining Dataset loss: 0.80\n",
      "Training Dataset acc: 79.56%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print \"Training Dataset %s: %.2f\"%(model.metrics_names[0], scores[1])\n",
    "print \"Training Dataset %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.8 Switch to GPU model\n",
    "\n",
    "\n",
    "### 7.8.1 For MacOS with Nvidia GPU\n",
    "\n",
    "[mac osx/linux下如何将keras运行在GPU上](http://blog.csdn.net/u014205968/article/details/50166651)\n",
    "\n",
    "Note: there are some question when install CUDA if your xcode version is 8.0 above, see [here](http://blog.cycleuser.org/use-cuda-80-with-macos-sierra-1012.html)\n",
    "\n",
    "使用下面这个脚本来验证是否启动GPU:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 2.961800 seconds\n",
      "Result is [ 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753\n",
      "  1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox  \n",
    "import theano.tensor as T  \n",
    "import numpy  \n",
    "import time  \n",
    "  \n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core  \n",
    "iters = 1000  \n",
    "  \n",
    "rng = numpy.random.RandomState(22)  \n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))  \n",
    "f = function([], T.exp(x))  \n",
    "print(f.maker.fgraph.toposort())  \n",
    "t0 = time.time()  \n",
    "for i in xrange(iters):  \n",
    "    r = f()  \n",
    "t1 = time.time()  \n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))  \n",
    "print(\"Result is %s\" % (r,))  \n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):  \n",
    "    print('Used the cpu')  \n",
    "else:  \n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
