{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7: Develop Your First Neural Network With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Overview\n",
    "\n",
    "Six steps:\n",
    "\n",
    "1. Load Data.\n",
    "2. Define Model.\n",
    "3. Compile Model.\n",
    "4. Fit Model.\n",
    "5. Evaluate Model.\n",
    "6. Tie It All Together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Get Data Set\n",
    "\n",
    "In this tutorial we are going to use the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "The introduction of this data set: [pima-indians-diabetes.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names)\n",
    "\n",
    "The data set file: [pima-indians-diabetes.data](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data)\n",
    "\n",
    "You can get the data by the fellowing step:\n",
    "\n",
    "```\n",
    "$ cd ./data_set/\n",
    "$ ./get_pima_indians_diabetes_data.sh\n",
    "```\n",
    "\n",
    "pima-indians-diabetes.data will be downloaded at data_set/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Load Data\n",
    "\n",
    "### Initialize random seed\n",
    "\n",
    "It is a good idea to initialize the random number generator with a fixed seed value. This is so that you can run the same code again and again and get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "\n",
    "Load data by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (768, 8) float64\n",
      "Y (768,) float64\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt(\"./data_set/pima-indians-diabetes.data\", delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "print 'X', X.shape, X.dtype\n",
    "print 'Y', Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Define Model\n",
    "\n",
    "Create a model by keras.models.Sequential and add the layers we designed:\n",
    "\n",
    "    The first hidden layer has 12 neurons and expects 8 input variables, and a relu activation.\n",
    "    The second hidden layer has 8 neurons, and a relu activation.\n",
    "    Finally the output layer has 1 neuron to predict the class, and a sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model\n",
    "\n",
    "Compiling the model uses the efficient numercial libraries (Tensorflow or Theano). In this step, we shoule specify some hyperperemeters for training process:\n",
    "\n",
    "    loss function: binary_crossentropy\n",
    "    optimizer: adam\n",
    "    metrics\" accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Fit Model\n",
    "\n",
    "Fit moel means training model, the peremeters in this step are:\n",
    "\n",
    "    nb_epoch: 150\n",
    "    batch_size: 10\n",
    "\n",
    "nb_epoch means the number of epoch, which fix the number of iterations. batch_size means the batch size in the method \"mini-batch gradient descent\"\n",
    "\n",
    "The training process is runing on your CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 0.6826 - acc: 0.6328     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.6590 - acc: 0.6510     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.6475 - acc: 0.6549     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6416 - acc: 0.6615     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6216 - acc: 0.6745     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6128 - acc: 0.6680     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6018 - acc: 0.6927     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.5962 - acc: 0.6927     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.5991 - acc: 0.6953     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.5920 - acc: 0.6927     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.5905 - acc: 0.6979     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.5883 - acc: 0.6901     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.5870 - acc: 0.6953     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.5869 - acc: 0.6836     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.5815 - acc: 0.6953     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.5779 - acc: 0.6966     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5809 - acc: 0.6849     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.5818 - acc: 0.6953     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5814 - acc: 0.6901     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5748 - acc: 0.7096     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5758 - acc: 0.7005     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5739 - acc: 0.7135     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5736 - acc: 0.6927     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5750 - acc: 0.6940     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5734 - acc: 0.7031     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5683 - acc: 0.7083     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5688 - acc: 0.7018     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5714 - acc: 0.7070     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5621 - acc: 0.7188     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5647 - acc: 0.7122     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5630 - acc: 0.7135     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5613 - acc: 0.7214     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5594 - acc: 0.7188     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5598 - acc: 0.7187     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5624 - acc: 0.7187     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5615 - acc: 0.7201     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5544 - acc: 0.7214     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5529 - acc: 0.7135     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5550 - acc: 0.7227     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5574 - acc: 0.7331     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5562 - acc: 0.7357     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5458 - acc: 0.7370     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5487 - acc: 0.7253     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5409 - acc: 0.7344     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5445 - acc: 0.7435     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5362 - acc: 0.7357     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5400 - acc: 0.7357     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5354 - acc: 0.7409     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5406 - acc: 0.7357     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5476 - acc: 0.7357     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5288 - acc: 0.7461     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5284 - acc: 0.7474     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5306 - acc: 0.7370     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5283 - acc: 0.7487     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5252 - acc: 0.7539     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5257 - acc: 0.7552     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5282 - acc: 0.7422     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5269 - acc: 0.7513     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5248 - acc: 0.7487     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5202 - acc: 0.7500     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5160 - acc: 0.7552     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5161 - acc: 0.7461     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5100 - acc: 0.7591     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5145 - acc: 0.7526     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5125 - acc: 0.7474     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5148 - acc: 0.7617     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5088 - acc: 0.7539     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5124 - acc: 0.7721     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5152 - acc: 0.7487     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5163 - acc: 0.7552     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5195 - acc: 0.7422     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5094 - acc: 0.7461     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.4984 - acc: 0.7617     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.4979 - acc: 0.7617     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5073 - acc: 0.7591     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5028 - acc: 0.7513     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7552     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.4929 - acc: 0.7578     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5011 - acc: 0.7630     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7474     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5026 - acc: 0.7578     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.4986 - acc: 0.7604     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.4948 - acc: 0.7578     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4915 - acc: 0.7747     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.4982 - acc: 0.7578     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7721     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4972 - acc: 0.7578     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.4921 - acc: 0.7695     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.4900 - acc: 0.7630     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.4870 - acc: 0.7669     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.4979 - acc: 0.7591     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.4943 - acc: 0.7682     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4907 - acc: 0.7630     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4859 - acc: 0.7617     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.4833 - acc: 0.7695     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4842 - acc: 0.7695     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4753 - acc: 0.7786     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4990 - acc: 0.7422     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4856 - acc: 0.7695     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4814 - acc: 0.7721     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4857 - acc: 0.7695     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4769 - acc: 0.7878     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4844 - acc: 0.7552     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4817 - acc: 0.7760     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.4803 - acc: 0.7760     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4811 - acc: 0.7812     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4836 - acc: 0.7773     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4774 - acc: 0.7826     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4775 - acc: 0.7786     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4795 - acc: 0.7695     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4713 - acc: 0.7747     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4745 - acc: 0.7826     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4751 - acc: 0.7773     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4808 - acc: 0.7617     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4790 - acc: 0.7643     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4723 - acc: 0.7786     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4668 - acc: 0.7799     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4644 - acc: 0.7839     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4711 - acc: 0.7839     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4694 - acc: 0.7839     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4760 - acc: 0.7839     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4725 - acc: 0.7747     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4699 - acc: 0.7826     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4701 - acc: 0.7799     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4695 - acc: 0.7747     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4642 - acc: 0.7799     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4719 - acc: 0.7656     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4579 - acc: 0.7865     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4644 - acc: 0.7865     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4735 - acc: 0.7760     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4606 - acc: 0.7773     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4639 - acc: 0.7760     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4751 - acc: 0.7865     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4729 - acc: 0.7773     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4601 - acc: 0.7917     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4564 - acc: 0.7852     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4625 - acc: 0.7852     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4684 - acc: 0.7760     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4605 - acc: 0.7734     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4590 - acc: 0.7839     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4602 - acc: 0.7734     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4593 - acc: 0.7760     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4614 - acc: 0.7878     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4508 - acc: 0.7969     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4580 - acc: 0.7747     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4627 - acc: 0.7812     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4531 - acc: 0.7943     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4656 - acc: 0.7734     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4566 - acc: 0.7839     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4593 - acc: 0.7839     \n",
      "Fit time Cost 7.5299179554 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# fit model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "end_time = time.time()\n",
    "print \"Fit time Cost %s s\"%(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.7 Evaluate Model\n",
    "\n",
    "In this part, we can calculate the accuracy fo this model on training dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0sTraining Dataset loss: 0.80\n",
      "Training Dataset acc: 79.56%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print \"Training Dataset %s: %.2f\"%(model.metrics_names[0], scores[1])\n",
    "print \"Training Dataset %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.8 Switch to GPU model\n",
    "\n",
    "\n",
    "### 7.8.1 For MacOS with Nvidia GPU\n",
    "\n",
    "[mac osx/linux下如何将keras运行在GPU上](http://blog.csdn.net/u014205968/article/details/50166651)\n",
    "\n",
    "Note: there are some question when install CUDA if your xcode version is 8.0 above, see [here](http://blog.cycleuser.org/use-cuda-80-with-macos-sierra-1012.html)\n",
    "\n",
    "使用下面这个脚本来验证是否启动GPU:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float32, vector)>)]\n",
      "Looping 1000 times took 2.074569 seconds\n",
      "Result is [ 1.23178029  1.61879337  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323284]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox  \n",
    "import theano.tensor as T  \n",
    "import numpy  \n",
    "import time  \n",
    "  \n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core  \n",
    "iters = 1000  \n",
    "  \n",
    "rng = numpy.random.RandomState(22)  \n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))  \n",
    "f = function([], T.exp(x))  \n",
    "print(f.maker.fgraph.toposort())  \n",
    "t0 = time.time()  \n",
    "for i in xrange(iters):  \n",
    "    r = f()  \n",
    "t1 = time.time()  \n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))  \n",
    "print(\"Result is %s\" % (r,))  \n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):  \n",
    "    print('Used the cpu')  \n",
    "else:  \n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
